{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[seminar]mlp_pytorch.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aaSoSTVpYJcu","colab_type":"text"},"source":["<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=300, height=300></p>\n","\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"GpZt9IYkYJcw","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"s4wplUzDYJcx","colab_type":"text"},"source":["<h2 style=\"text-align: center;\"><b>Многослойная сеть на PyTorch</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"J2msuyHTYJcx","colab_type":"text"},"source":["В этом ноутбке мы научимся писать свои нейросети на фреймворке PyTorch, конкретно - рассмотрим, как написать многослойную полносвязную сеть (Fully-Connected, FC), и сравним их качество на датасете картинок MNIST."]},{"cell_type":"markdown","metadata":{"id":"9xJnMEZrYJcz","colab_type":"text"},"source":["<h3 style=\"text-align: center;\"><b>Компоненты нейросети</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"InwacmvIYJc0","colab_type":"text"},"source":["Здесь самое время напомнить о том, какие вещи играют принципиальную роль в построении любой ***нейронной сети*** (все их мы задаём *руками*, самостоятельно):  \n","\n","- непосредственно, сама **архитектура** нейросети (сюда входят типы функций активации у каждого нейрона);\n","- начальная **инициализация** весов каждого слоя;\n","- метод **оптимизации** нейросети (сюда ещё входит метод изменения `learning_rate`);\n","- размер **батчей** (`batch_size`);\n","- количество итераций обучения (`num_epochs`);\n","- **функция потерь** (`loss`);  \n","- тип **регуляризации** нейросети (для каждого слоя можно свой);  \n","\n","То, что связано с ***данными и задачей***:  \n","- само **качество** выборки (непротиворечивость, чистота, корректность постановки задачи);  \n","- **размер** выборки;  "]},{"cell_type":"markdown","metadata":{"id":"tXujEOB0YJc1","colab_type":"text"},"source":["<h3 style=\"text-align: center;\"><b>Многослойная нейронная сеть</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"mnxH-DajYJc3","colab_type":"text"},"source":["Как можно понять из названия, многослойная нейросеть состоит из нескольких **слоёв**. Каждый слой состоит из **нейронов**. Ранее мы уже писали свой нейрон на NumPy, вот из таких нейронов и состоит ***MLP (Multi-Layer Perceptron)***. Ещё такую многослойную нейросеть, у которой каждый нейрон на предыдущем уровне соединён с нейроном на следующем уровне, называют ***Fully-Connected-сетью*** (или ***Dense-сетью***).  \n","\n","Расмотрим их устройство более подробно:"]},{"cell_type":"markdown","metadata":{"id":"onjJUneMYJc5","colab_type":"text"},"source":["* Вот так выглядит двухслойная нейросеть (первый слой - input layer - не считается, потому что это, по сути, не слой):"]},{"cell_type":"markdown","metadata":{"id":"owRRulLzYJc6","colab_type":"text"},"source":["<img src=\"http://cs231n.github.io/assets/nn1/neural_net.jpeg\" width=300, height=200>"]},{"cell_type":"markdown","metadata":{"id":"tFNxGGBEYJc8","colab_type":"text"},"source":["* Так выглядит трёхслойная нейросеть:"]},{"cell_type":"markdown","metadata":{"id":"zRaKX35eYJc9","colab_type":"text"},"source":["<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" width=400, height=300>"]},{"cell_type":"markdown","metadata":{"id":"6w1FTkO1YJc-","colab_type":"text"},"source":[".. и так далее для большего случая слоёв."]},{"cell_type":"markdown","metadata":{"id":"8iKV7m5YYJc_","colab_type":"text"},"source":["**Обратите внимание:** связи есть у нейронов со слоя $L_{i-1}$  и нейронов $L_{i}$, но между нейронами в одном слое связей **нет**."]},{"cell_type":"markdown","metadata":{"id":"URV9qWkfYJdA","colab_type":"text"},"source":["**Входной слой** -- это данные (матрица $(n, m)$)."]},{"cell_type":"markdown","metadata":{"id":"zK8tWuHHYJdB","colab_type":"text"},"source":["Слои, которые не являются входными или выходными, называются **скрытыми слоями (hidden layers)**."]},{"cell_type":"markdown","metadata":{"id":"fz9clUlCYJdC","colab_type":"text"},"source":["При решении ***задачи регрессии*** на **выходном слое** обычно один нейрон, который возвращает предсказанные числа (для каждого объекта по числу).  \n","\n","В случае ***задачи классификации*** на **выходном слое** обычно один нейрон, если задача бинарной классификации, и $K$ нейронов, если задача $K$-класовой классификации."]},{"cell_type":"markdown","metadata":{"id":"nJblXqY5YJdE","colab_type":"text"},"source":["#### Forward pass в MLP"]},{"cell_type":"markdown","metadata":{"id":"D87xoAl8YJdF","colab_type":"text"},"source":["Каждый слой многослойной нейросети - это матрица весов, строки которой - это нейроны (одна строка - один нейрон), а столбцы - веса каждого нейрона (то есть одна строка - это веса одного нейрона)."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"RyXUqCfVYJdG","colab_type":"text"},"source":["Допустим, мы решаем задачу $K$-классовой классификации (на последнем слое $K$ нейронов). Рассмотрим, как в таком случае выглядит `forward_pass` нейросети:"]},{"cell_type":"markdown","metadata":{"id":"YO6gHbOjYJdH","colab_type":"text"},"source":["* Вход: $$X =\n","\\left(\n","\\begin{matrix} \n","x_{11} & ... & x_{1M} \\\\\n","... & \\ddots  & ...\\\\\n","x_{N1} & ... & x_{NM} \n","\\end{matrix}\n","\\right)\n","$$\n","\n","-- матрица $(N, M)$"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"XdImZiQkYJdI","colab_type":"text"},"source":["* Структура сети - много слоёв, в слоях много нейронов. Первый слой (после входного) выглядит так:"]},{"cell_type":"markdown","metadata":{"id":"U2c2M4MJYJdJ","colab_type":"text"},"source":["$$ W^1 =\n","\\left(\n","\\begin{matrix} \n","w_{11} & ... & w_{1L_1} \\\\\n","... & \\ddots  & ...\\\\\n","w_{M1} & ... & w_{ML_1} \n","\\end{matrix}\n","\\right)\n","$$\n","\n","-- матрица $(M, L_1)$"]},{"cell_type":"markdown","metadata":{"id":"UUCdeLN0YJdK","colab_type":"text"},"source":["То есть это в точности $L_1$ нейронов, каждый имеет свои собственные веса, их $M$ штук."]},{"cell_type":"markdown","metadata":{"id":"ixHtlMwKYJdL","colab_type":"text"},"source":["Мы помним, что нейрон - это линейное преобразование и потом нелинейная функция активации от этого преобразования. Однако в многослойных нейростеях часто отделяют `Linear` часть и `Activation`, то есть слоем считаем набор весов нейронов, а следующий слой всегда функция активации (у всех нейронов из слоя она одна и та же, обычно фреймворки не позволяют задавать конкретному нейрону в слое отличную от других нейронов в этом слое функцию активации, однако это легко сделать, объявив слой из одного нейрона)."]},{"cell_type":"markdown","metadata":{"id":"AUt1NgTvYJdN","colab_type":"text"},"source":["* Другие слои выглядит точно так же, как первый слой. Например, у второй слой будет такой:"]},{"cell_type":"markdown","metadata":{"id":"IdtSTvTmYJdN","colab_type":"text"},"source":["$$ W^2 =\n","\\left(\n","\\begin{matrix} \n","w_{11} & ... & w_{1L_2} \\\\\n","... & \\ddots  & ...\\\\\n","w_{L_11} & ... & w_{L_1L_2} \n","\\end{matrix}\n","\\right)\n","$$\n","\n","-- матрица $(L_1, L_2)$"]},{"cell_type":"markdown","metadata":{"id":"R3nGnHoLYJdP","colab_type":"text"},"source":["То есть это в точности $L_2$ нейронов, каждый имеет свои собственные веса, их $L_1$ штук."]},{"cell_type":"markdown","metadata":{"id":"bQhOAQjSYJdR","colab_type":"text"},"source":["* Выходной слой:  \n","\n","Пусть в нейросети до выходного слоя идут $t$ слоёв. Тогда выходной слой имеет форму:"]},{"cell_type":"markdown","metadata":{"id":"fWvqm-K0YJdT","colab_type":"text"},"source":["$$ W^{out} =\n","\\left(\n","\\begin{matrix} \n","w_{11} & ... & w_{1K} \\\\\n","... & \\ddots  & ...\\\\\n","w_{L_t1} & ... & w_{L_tK} \n","\\end{matrix}\n","\\right)\n","$$\n","\n","-- матрица $(L_t, K)$, где $L_t$ - количество нейронов в $t$-ом слое, а $K$ -- количество классов."]},{"cell_type":"markdown","metadata":{"id":"2z5tO89NYJdU","colab_type":"text"},"source":["В итоге ***для `forward_pass` нам нужно просто последовтельно перемножить матрицы друг за другом, применяя после каждого умножения соответсвующую функцию активации***."]},{"cell_type":"markdown","metadata":{"id":"IZT4GgsCYJdV","colab_type":"text"},"source":["*Примечание*: можно думать об умножении на очередную матрицу весов как на переход в **новое признаковое пространство**. Действительно, когда подаём матрицу $X$ и умножаем на матрицу первого слоя, мы получаем матрицу размера $(N, L_1)$, то есть как будто $L_1$ \"новых\" признаков (построенных как линейная комбинация старых до применения функции активации, и уже как нелинейная комбинация после активации)."]},{"cell_type":"markdown","metadata":{"id":"4RhJ4fsHYJdW","colab_type":"text"},"source":["**Backward pass в MLP**"]},{"cell_type":"markdown","metadata":{"id":"MYN043DbYJdX","colab_type":"text"},"source":["Обучается с помощью метода \"Error Backpropagation\" - [\"Обратное распространение ошибки\"](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8), принцип распространения очень похож на то, как мы обучали один нейрон - это градиентный спуск, но по \"всей нейросети\" сразу.  "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"oK7Vi4bxYJdZ","colab_type":"text"},"source":["Backpropagation работает корректно благодаря ***chain rule*** (=правилу взятия производной сложной функции):  \n","\n","Если $f(x) = f(g(x))$, то:  \n","\n","$$\\frac{\\partial{f}}{\\partial{x}} = \\frac{\\partial{f}}{\\partial{g}} \\frac{\\partial{g}}{\\partial{x}}$$"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"WKvMsaEBYJda","colab_type":"text"},"source":["Более подробно про backpropagation можно прочитать здесь (на английском):  https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"]},{"cell_type":"markdown","metadata":{"id":"z5dyjPVNYJdc","colab_type":"text"},"source":["<h3 style=\"text-align: center;\"><b>Многослойная нейросеть на PyTorch</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"T9ufk3ECYJde","colab_type":"text"},"source":["Ешё раз напомним про основные компоненты нейросети:\n","\n","- непосредственно, сама **архитектура** нейросети (сюда входят типы функций активации у каждого нейрона);\n","- начальная **инициализация** весов каждого слоя;\n","- метод **оптимизации** нейросети (сюда ещё входит метод изменения `learning_rate`);\n","- размер **батчей** (`batch_size`);\n","- количетсво итераций обучения (`num_epochs`);\n","- **функция потерь** (`loss`);  \n","- тип **регуляризации** нейросети (для каждого слоя можно свой);  \n","\n","То, что связано с ***данными и задачей***:  \n","- само **качество** выборки (непротиворечивость, чистота, корректность постановки задачи);  \n","- **размер** выборки;  "]},{"cell_type":"markdown","metadata":{"id":"9KrWarqTYJdf","colab_type":"text"},"source":["Не будем медлить - бахнем 100 нейронов в двуслойную нейросеть (датасет - снова \"Игрушка дьявола\"):"]},{"cell_type":"code","metadata":{"id":"bLjkPg19YJdg","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCDVRvQJYJdl","colab_type":"text"},"source":["Генерация и отрисовка датасета:"]},{"cell_type":"code","metadata":{"id":"k0J27RcLYJdm","colab_type":"code","colab":{}},"source":["# код для генерации взят из Стэнфордсокго курса:\n","# http://cs231n.github.io/neural-networks-case-study/#linear\n","\n","N = 100\n","D = 2\n","K = 3\n","X = np.zeros((N * K, D))\n","y = np.zeros(N * K, dtype='uint8')\n","\n","for j in range(K):\n","    ix = range(N * j,N * (j + 1))\n","    r = np.linspace(0.0, 1, N)\n","    t = np.linspace(j * 4, (j + 1) * 4,N) + np.random.randn(N) * 0.2 # theta\n","    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n","    y[ix] = j"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9FHhqX_YJdp","colab_type":"text"},"source":["Не забываем оборачивать данные (без этого градиенты не посчитать):"]},{"cell_type":"code","metadata":{"id":"pQINaQqZYJdq","colab_type":"code","colab":{}},"source":["X = torch.autograd.Variable(torch.FloatTensor(X))\n","y = torch.autograd.Variable(torch.LongTensor(y.astype(np.int64)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Who9mS8oYJdu","colab_type":"code","outputId":"1ec6e30a-2cd9-4bd6-d0e0-3b7c936d1b8a","colab":{}},"source":["print(X.data.shape, y.data.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([300, 2]) torch.Size([300])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I-9dFW5CYJd0","colab_type":"text"},"source":["Сама ячейка с сеткой и обучением:"]},{"cell_type":"code","metadata":{"id":"kf-YapleYJd1","colab_type":"code","colab":{}},"source":["# пример взят из официального туториала: \n","# https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n","\n","# N - размер батча (batch_size, нужно для метода оптимизации); \n","# D_in - размерность входа (количество признаков у объекта);\n","# H - размерность скрытых слоёв; \n","# D_out - размерность выходного слоя (суть - количество классов)\n","N, D_in, H, D_out = 64, 2, 100, 3\n","\n","# Use the nn package to define our model and loss function.\n","two_layer_net = torch.nn.Sequential(\n","    torch.nn.Linear(D_in, H),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(H, D_out),\n",")\n","\n","loss_fn = torch.nn.CrossEntropyLoss(size_average=False)\n","\n","learning_rate = 1e-4\n","optimizer = torch.optim.SGD(two_layer_net.parameters(), lr=learning_rate)\n","for t in range(500):\n","    # forward\n","    y_pred = two_layer_net(X)\n","\n","    # loss\n","    loss = loss_fn(y_pred, y)\n","    print('{} {}'.format(t, loss.data))\n","\n","    # ЗАНУЛЯЕМ!\n","    optimizer.zero_grad()\n","\n","    # backward\n","    loss.backward()\n","\n","    # ОБНОВЛЯЕМ! \n","    optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTQiN0bcYJd6","colab_type":"text"},"source":["**Обратите внимание:** несмотря на то, что это задача 3-х классовой классификации и столбец $y$ нужно по-хорошему кодировать OneHotEncoding'ом, мы подали просто столбец из 0, 1 и 2 и всё отработало. Вывод - PyTorch сам делает OneHot в таком случае."]},{"cell_type":"markdown","metadata":{"id":"apsWWq17YJd8","colab_type":"text"},"source":["Проверим, насколько хороша наша сеть из 100 нейронов:"]},{"cell_type":"code","metadata":{"id":"X0ICB6Z-YJd-","colab_type":"code","colab":{}},"source":["# Обратно в Numpy для отрисовки\n","X = X.data.numpy()\n","y = y.data.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JvjfvgbPYJeB","colab_type":"code","colab":{}},"source":["# Отрисовочная магия, снова взято из:\n","# http://cs231n.github.io/neural-networks-case-study/#linear\n","\n","h = 0.02\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","grid_tensor = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n","\n","Z = two_layer_net(torch.autograd.Variable(grid_tensor))\n","Z = Z.data.numpy()\n","Z = np.argmax(Z, axis=1)\n","Z = Z.reshape(xx.shape)\n","\n","plt.figure(figsize=(10, 8))\n","\n","plt.contourf(xx, yy, Z, cmap=plt.cm.rainbow, alpha=0.3)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.rainbow)\n","\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","\n","plt.title('Игрушка дьявола', fontsize=15)\n","plt.xlabel('$x$', fontsize=14)\n","plt.ylabel('$y$', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-LuUcmNYJeE","colab_type":"text"},"source":["Качество не очень. Как же так, ведь мы использовали 100 нейронов? Разве их мало?"]},{"cell_type":"markdown","metadata":{"id":"CUh5va_PYJeF","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"34q5qIBvYJeG","colab_type":"text"},"source":["### Задача 2"]},{"cell_type":"markdown","metadata":{"id":"G2wNgoh5YJeG","colab_type":"text"},"source":["Улучшите сеть (помните про вещи, которые можно менять (см. Компоненты нейросети)). Экспериментируйте, в этом **вся суть deep learning** (и в том, чтобы рано или поздно сетка научилась экспериментировать за Вас :)"]},{"cell_type":"code","metadata":{"id":"ZYzvOiPgYJeH","colab_type":"code","colab":{}},"source":["# Ваш код здесь (можно (и нужно) создавать больше ячеек)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6N_9wfvPYJeK","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"H_thmQJOYJeK","colab_type":"text"},"source":["<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"RpSrLf9FYJeL","colab_type":"text"},"source":["1). *Примеры написания нейросетей на PyTorch (офийиальные туториалы) (на английском): https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples  \n","https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html*\n","\n","2). ***Один из самых подробных и полных курсов по deep learning на данный момент - это курс Стэнфордского Университета (он вообще сейчас один из лидеров в области ИИ, его выпускники работают в Google, Facebook, Amazon, Microsoft, в стартапах в Кремниевой долине):  http://cs231n.github.io/***  \n","\n","3). Практически исчерпывающая информация по основам нейросетей (из cs231n) (на английском):  \n","\n","http://cs231n.github.io/neural-networks-1/,  \n","http://cs231n.github.io/neural-networks-2/,  \n","http://cs231n.github.io/neural-networks-3/,  \n","http://cs231n.github.io/neural-networks-case-study/#linear\n","\n","4). *Хорошие статьи по основам нейросетей (на английском):  http://neuralnetworksanddeeplearning.com/chap1.html*\n","\n","5). *Наглядная демонстрация того, как обучаются нейросети:  https://cs.stanford.edu/people/karpathy/convnetjs/*"]},{"cell_type":"markdown","metadata":{"id":"1Qldb1U5YJeM","colab_type":"text"},"source":["6). *Подробнее про backprop -- статья на Medium: https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670*"]},{"cell_type":"markdown","metadata":{"id":"g1HSslRhYJeN","colab_type":"text"},"source":["7). *Статья из интернет по Backprop: http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf*"]}]}